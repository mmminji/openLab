{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7_CIFAR-10_with_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cdd0cc73edf4a76be0903e9953677c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a466818b551413d841fd074b99f1979",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a293a78710a4ba2a219c93fda80c532",
              "IPY_MODEL_5dc56507bdc54edcb7906210732afe72"
            ]
          }
        },
        "9a466818b551413d841fd074b99f1979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a293a78710a4ba2a219c93fda80c532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c3df0710ef3844618edae83c796e6c3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_767861154a9d4075aedafb49e02aa349"
          }
        },
        "5dc56507bdc54edcb7906210732afe72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b54e1ec377942268f0747fb06f5614a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 52306995.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03088b5a047144ed9ce2bfe712c15e66"
          }
        },
        "c3df0710ef3844618edae83c796e6c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "767861154a9d4075aedafb49e02aa349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b54e1ec377942268f0747fb06f5614a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03088b5a047144ed9ce2bfe712c15e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoWtdW8jFNgj"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPWEXDj1GZwi"
      },
      "source": [
        "import torch\n",
        "import torchvision  #비전관련 library\n",
        "import torchvision.transforms as transforms  #데이터 조작\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F  #Neural Network 구조 construct\n",
        "import torch.optim as optim  #최적화\n",
        "import argparse  #hyper-parameter셋팅\n",
        "import numpy as np  #배열연산\n",
        "import time #시간계산\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns  #시각화\n",
        "import matplotlib.pyplot as plt #시각화"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9osn1HVGgxU",
        "outputId": "33e70700-01fe-4e4f-f197-ce617d704099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "8cdd0cc73edf4a76be0903e9953677c7",
            "9a466818b551413d841fd074b99f1979",
            "2a293a78710a4ba2a219c93fda80c532",
            "5dc56507bdc54edcb7906210732afe72",
            "c3df0710ef3844618edae83c796e6c3f",
            "767861154a9d4075aedafb49e02aa349",
            "3b54e1ec377942268f0747fb06f5614a",
            "03088b5a047144ed9ce2bfe712c15e66"
          ]
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# train data를 4:1로 val로 나눔\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cdd0cc73edf4a76be0903e9953677c7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A2DCvUxGnsb"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        if self.use_xavier:\n",
        "            self.xavier_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.act(self.linears[i](x))\n",
        "            if self.use_bn:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiyev8moQIEl"
      },
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US83BQgZQT5r"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_code, in_channels, out_dim, act, use_bn): \n",
        "        super(CNN, self).__init__() \n",
        "        \n",
        "        if act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        elif act == 'tanh':\n",
        "            self.act = nn.TanH()\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid activation function code\")\n",
        "        \n",
        "        self.layers = self._make_layers(model_code, in_channels, use_bn)   # private function 여기서만 사용할 함수\n",
        "        self.classifer = nn.Sequential(nn.Linear(512, 256), # fully connected layer\n",
        "                                       self.act,\n",
        "                                       nn.Linear(256, out_dim))   # linear layer를 (여러개 쌓음 fc마지막에 두개 붙이듯이) / list 형식 아니면 *안붙여도 됨 / linear 후엔 activation, 마지막엔 softmax 들어가기전\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        # # nn.Sequential 안했을 경우\n",
        "        # for layer in layers:\n",
        "        #   x = layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifer(x)\n",
        "        return x\n",
        "        \n",
        "    def _make_layers(self, model_code, in_channels, use_bn):  # 함수 부를 때도 넣어줘야함(self.layers) / 생성자에서 받아와야함(__init__)\n",
        "        layers = []\n",
        "        for x in cfg[model_code]:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels=in_channels,\n",
        "                                     out_channels=x,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)]\n",
        "                if use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)  # list에 layer object를 넣어주고 nn.Sequential을 하면 sub 모듈을 만들어주는 효과 / nn.Sequnetiail을 안하면  for문으로 하나씩 layer 넣어줘야함"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRX93b0AHo_Q",
        "outputId": "d1839e3d-9f2f-4edf-e00e-c29276d0faff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# class CNN1(nn.Module):\n",
        "    \n",
        "#     def __init__(self): # construct\n",
        "#         super(CNN1, self).__init__()       #상속 \n",
        "        \n",
        "#         # object 생성\n",
        "#         self.conv1 = nn.Conv2d(in_channels = 3,\n",
        "#                                out_channels = 64,\n",
        "#                                kernel_size = 3,\n",
        "#                                stride = 1,\n",
        "#                                padding = 1)  # kernel_size/2로 하면 크기 유지됨\n",
        "#         self.conv2 = nn.Conv2d(in_channels = 64,\n",
        "#                                out_channels = 256,\n",
        "#                                kernel_size = 5,\n",
        "#                                stride = 1,\n",
        "#                                padding = 2)\n",
        "#         self.act = nn.ReLU()   # 파라미터가 없기 때문에 여러개를 만들어주지 않아도 됨\n",
        "#         self.maxpool1 = nn.MaxPool2d(kernel_size = 2,\n",
        "#                                      stride = 2)   # 절반으로 줄여줌 \n",
        "#         self.fc = nn.Linear(65536, 10)  # 일자로 폈을 때 65536d / 10 class\n",
        "\n",
        "#     def forward(self, x):   # 실제 사용\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.act(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.act(x)\n",
        "#         x = self.maxpool1(x)\n",
        "#         x = x.view(x.size(0), -1)  # linear를 넣기 전에는 2d를 1d로 / x.size(0):batch size / -1:나머지 일자로\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# def dimension_check():\n",
        "#     net = CNN1()\n",
        "#     x = torch.randn(2, 3, 32, 32)\n",
        "#     y = net(x)\n",
        "#     print(y.size())\n",
        "\n",
        "# dimension_check()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB9tQzCUihP0",
        "outputId": "770494ea-9874-4f9d-9512-7cad76802242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# class CNN2(nn.Module):\n",
        "    \n",
        "#     def __init__(self, model_code, in_channels): \n",
        "#         super(CNN2, self).__init__() \n",
        "        \n",
        "#         self.layers = self._make_layers(model_code, in_channels)   # private function 여기서만 사용할 함수\n",
        "#         self.classifer = nn.Sequential(nn.Linear(512, 256),\n",
        "#                                        nn.ReLU(),\n",
        "#                                        nn.Linear(256, 10))   # linear layer를 (여러개 쌓음 fc마지막에 두개 붙이듯이) / list 형식 아니면 *안붙여도 됨 / linear 후엔 activation, 마지막엔 softmax 들어가기전\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = self.layers(x)\n",
        "#         # # nn.Sequential 안했을 경우\n",
        "#         # for layer in layers:\n",
        "#         #   x = layer(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.classifer(x)\n",
        "#         return x\n",
        "        \n",
        "#     def _make_layers(self, model_code, in_channels):  # 함수 부를 때도 넣어줘야함(self.layers) / 생성자에서 받아와야함(__init__)\n",
        "#         layers = []\n",
        "#         for x in cfg[model_code]:\n",
        "#             if x == 'M':\n",
        "#                 layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "#             else:\n",
        "#                 layers += [nn.Conv2d(in_channels=in_channels,\n",
        "#                                      out_channels=x,\n",
        "#                                      kernel_size=3,\n",
        "#                                      stride=1,\n",
        "#                                      padding=1)]\n",
        "#                 layers += [nn.ReLU()]\n",
        "#                 in_channels = x\n",
        "#         return nn.Sequential(*layers)  # list에 layer object를 넣어주고 nn.Sequential을 하면 sub 모듈을 만들어주는 효과 / nn.Sequnetiail을 안하면  for문으로 하나씩 layer 넣어줘야함\n",
        "\n",
        "# def dimension_check():\n",
        "#     net = CNN2('VGG11', 3)\n",
        "#     x = torch.randn(2, 3, 32, 32)\n",
        "#     y = net(x)\n",
        "#     print(y.size())\n",
        "\n",
        "# dimension_check()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bI6A9QxLjtJ"
      },
      "source": [
        "# 만든 CNN이 차원이 맞는지 확인\n",
        "def dimension_check():\n",
        "    net = CNN('VGG11', 3)\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.size())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz9_iCJWGzGH"
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsS6PQvKG17w"
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imj5-XqbG4tI"
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_vScpPG61S"
      },
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn)\n",
        "    # net = CNN1()\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S8kDvhG9zk"
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Fc45ykHAnp"
      },
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "    \n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "\n",
        "    \n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmL0mGJpHEKh"
      },
      "source": [
        "# # ====== Random Seed Initialization ====== #\n",
        "# seed = 123\n",
        "# np.random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# args = parser.parse_args(\"\")\n",
        "# args.exp_name = \"exp1_lr_model_code\"\n",
        "\n",
        "# # ====== Model ====== #z\n",
        "# args.out_dim = 10\n",
        "# args.act = 'relu'\n",
        "\n",
        "# # ====== Regularization ======= #\n",
        "# args.l2 = 0.00001\n",
        "\n",
        "# # ====== Optimizer & Training ====== #\n",
        "# args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "# args.lr = 0.0015\n",
        "# args.epoch = 10\n",
        "\n",
        "# args.train_batch_size = 512\n",
        "# args.test_batch_size = 1024\n",
        "\n",
        "# # ====== Experiment Variable ====== #\n",
        "# name_var1 = 'lr'\n",
        "# name_var2 = 'l2'\n",
        "# list_var1 = [0.0001]\n",
        "# list_var2 = [0.00001]\n",
        "\n",
        "\n",
        "# for var1 in list_var1:\n",
        "#     for var2 in list_var2:\n",
        "#         setattr(args, name_var1, var1)\n",
        "#         setattr(args, name_var2, var2)\n",
        "#         print(args)\n",
        "                \n",
        "#         setting, result = experiment(partition, deepcopy(args))\n",
        "#         save_exp_result(setting, result)\n",
        "\n",
        "#         # 오래걸리는 이유\n",
        "#         # cuda에 안올려서 GPU를 사용안하고 있다\n",
        "#         # maxpooling을 한번밖에 안해서 parameter가 너무 크다 --> 깊게 쌓고 maxpooling을 여러번 써서 fc에 넣는 차원을 줄이자"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Los1Y3ZSmYOz",
        "outputId": "3a6c6a7d-8835-47b5-d9b8-7a1438eebb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr_model_code\"\n",
        "\n",
        "# ====== Model ====== #\n",
        "args.model_code = 'VGG11'\n",
        "args.in_channels = 3\n",
        "args.out_dim = 10\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.00001\n",
        "args.use_bn = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 512\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'lr'\n",
        "name_var2 = 'model_code'\n",
        "list_var1 = [0.0001, 0.00001]\n",
        "list_var2 = ['VGG11', 'VGG13']\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        # save_exp_result(setting, result)\n",
        "\n",
        "        # 오래걸리는 이유\n",
        "        # cuda에 안올려서 GPU를 사용안하고 있다\n",
        "        # maxpooling을 한번밖에 안해서 parameter가 너무 크다 --> 깊게 쌓고 maxpooling을 여러번 써서 fc에 넣는 차원을 줄이자"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', epoch=10, exp_name='exp1_lr_model_code', in_channels=3, l2=1e-05, lr=0.0001, model_code='VGG11', optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=512, use_bn=True)\n",
            "Epoch 0, Acc(train/val): 38.16/32.39, Loss(train/val) 1.59/2.10. Took 10.47 sec\n",
            "Epoch 1, Acc(train/val): 51.76/53.82, Loss(train/val) 1.28/1.26. Took 10.52 sec\n",
            "Epoch 2, Acc(train/val): 60.92/56.18, Loss(train/val) 1.07/1.23. Took 10.48 sec\n",
            "Epoch 3, Acc(train/val): 66.51/57.82, Loss(train/val) 0.93/1.21. Took 10.52 sec\n",
            "Epoch 4, Acc(train/val): 70.65/61.85, Loss(train/val) 0.81/1.14. Took 10.38 sec\n",
            "Epoch 5, Acc(train/val): 74.33/67.58, Loss(train/val) 0.73/0.94. Took 10.38 sec\n",
            "Epoch 6, Acc(train/val): 77.33/70.36, Loss(train/val) 0.64/0.88. Took 10.33 sec\n",
            "Epoch 7, Acc(train/val): 80.15/70.50, Loss(train/val) 0.57/0.88. Took 10.28 sec\n",
            "Epoch 8, Acc(train/val): 81.79/69.39, Loss(train/val) 0.53/0.98. Took 10.27 sec\n",
            "Epoch 9, Acc(train/val): 84.47/72.94, Loss(train/val) 0.45/0.87. Took 10.27 sec\n",
            "Namespace(act='relu', epoch=10, exp_name='exp1_lr_model_code', in_channels=3, l2=1e-05, lr=0.0001, model_code='VGG13', optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=512, use_bn=True)\n",
            "Epoch 0, Acc(train/val): 34.11/29.46, Loss(train/val) 1.66/2.44. Took 11.30 sec\n",
            "Epoch 1, Acc(train/val): 47.62/47.05, Loss(train/val) 1.37/1.49. Took 11.24 sec\n",
            "Epoch 2, Acc(train/val): 59.15/55.68, Loss(train/val) 1.11/1.27. Took 11.25 sec\n",
            "Epoch 3, Acc(train/val): 66.04/62.84, Loss(train/val) 0.94/1.05. Took 11.25 sec\n",
            "Epoch 4, Acc(train/val): 69.63/59.52, Loss(train/val) 0.85/1.23. Took 11.29 sec\n",
            "Epoch 5, Acc(train/val): 73.09/67.91, Loss(train/val) 0.76/0.91. Took 11.27 sec\n",
            "Epoch 6, Acc(train/val): 76.86/71.84, Loss(train/val) 0.67/0.82. Took 11.25 sec\n",
            "Epoch 7, Acc(train/val): 79.74/69.84, Loss(train/val) 0.59/0.91. Took 11.37 sec\n",
            "Epoch 8, Acc(train/val): 82.14/63.35, Loss(train/val) 0.52/1.18. Took 11.22 sec\n",
            "Epoch 9, Acc(train/val): 83.62/72.23, Loss(train/val) 0.48/0.88. Took 11.23 sec\n",
            "Namespace(act='relu', epoch=10, exp_name='exp1_lr_model_code', in_channels=3, l2=1e-05, lr=1e-05, model_code='VGG11', optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=512, use_bn=True)\n",
            "Epoch 0, Acc(train/val): 51.13/54.58, Loss(train/val) 1.35/1.23. Took 10.09 sec\n",
            "Epoch 1, Acc(train/val): 65.94/65.59, Loss(train/val) 0.95/0.96. Took 10.32 sec\n",
            "Epoch 2, Acc(train/val): 73.90/64.11, Loss(train/val) 0.75/1.03. Took 10.38 sec\n",
            "Epoch 3, Acc(train/val): 80.91/67.91, Loss(train/val) 0.56/0.93. Took 10.13 sec\n",
            "Epoch 4, Acc(train/val): 86.58/65.69, Loss(train/val) 0.41/1.13. Took 10.27 sec\n",
            "Epoch 5, Acc(train/val): 92.20/66.97, Loss(train/val) 0.25/1.19. Took 10.37 sec\n",
            "Epoch 6, Acc(train/val): 95.47/66.59, Loss(train/val) 0.15/1.28. Took 10.21 sec\n",
            "Epoch 7, Acc(train/val): 96.91/67.24, Loss(train/val) 0.11/1.27. Took 10.12 sec\n",
            "Epoch 8, Acc(train/val): 97.36/67.81, Loss(train/val) 0.08/1.33. Took 10.22 sec\n",
            "Epoch 9, Acc(train/val): 98.12/67.49, Loss(train/val) 0.06/1.41. Took 10.23 sec\n",
            "Namespace(act='relu', epoch=10, exp_name='exp1_lr_model_code', in_channels=3, l2=1e-05, lr=1e-05, model_code='VGG13', optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=512, use_bn=True)\n",
            "Epoch 0, Acc(train/val): 48.38/57.82, Loss(train/val) 1.41/1.16. Took 11.29 sec\n",
            "Epoch 1, Acc(train/val): 63.45/62.16, Loss(train/val) 1.01/1.07. Took 11.27 sec\n",
            "Epoch 2, Acc(train/val): 72.39/66.82, Loss(train/val) 0.78/0.97. Took 11.28 sec\n",
            "Epoch 3, Acc(train/val): 79.38/67.07, Loss(train/val) 0.60/0.96. Took 11.24 sec\n",
            "Epoch 4, Acc(train/val): 85.93/67.39, Loss(train/val) 0.43/1.02. Took 11.20 sec\n",
            "Epoch 5, Acc(train/val): 91.14/69.31, Loss(train/val) 0.28/1.02. Took 11.16 sec\n",
            "Epoch 6, Acc(train/val): 94.89/69.41, Loss(train/val) 0.17/1.10. Took 11.29 sec\n",
            "Epoch 7, Acc(train/val): 96.62/64.79, Loss(train/val) 0.11/1.52. Took 11.22 sec\n",
            "Epoch 8, Acc(train/val): 97.51/68.05, Loss(train/val) 0.08/1.33. Took 11.24 sec\n",
            "Epoch 9, Acc(train/val): 97.50/62.68, Loss(train/val) 0.08/1.74. Took 11.34 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAaWEHW0D-X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}