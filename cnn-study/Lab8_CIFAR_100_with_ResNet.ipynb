{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab8_CIFAR_100_with_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heartcored98/Standalone-DeepLearning/blob/master/Lab8_CIFAR_100_with_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i--BnHsc_WPv"
      },
      "source": [
        "# Lab8. CIFAR-100 Classification with ResNet\n",
        "\n",
        "Lab8에서는 assignment4-CIFAR-10 classification with CNN코드의 model architecture 부분을 수정하여 ResNet architecture를 통해 CIFAR-100 classification을 해보겠습니다. \n",
        "\n",
        "[ResNet implementation with Pytorch](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)를 참고하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BljO3R38_0yD"
      },
      "source": [
        "아래 명령을 통하여 Colab 서버 컴퓨터 내에 결과들을 저장할 results 폴더를 생성합니다. 이미 생성되어 있는 경우 `mkdir: cannot create directory 'results': File exists`와 같은 에러가 발생하나, 폴더가 이미 존재한다면 상관없으니 넘어가 줍시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuAvKKD5_CG0"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8J03tMz_Nku"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj0CoQLUAQkR"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "기존 CIFAR-10 데이터 저장 코드에서 10을 100으로 바꿔주기만 하면 CIFAR-100 dataset을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U0zaT67_Plf",
        "outputId": "68385ddc-7a1e-4142-8362-b052cc4a691d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix5IJ8boAb1-"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "[ResNet implementation with Pytorch](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)를 참고하여 ResNet architecture를 구현해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9N12-ITAtCv"
      },
      "source": [
        "### conv3x3 and conv1x1 functions\n",
        "자주 사용하게 될 1x1과 3x3 filter convolutional layer는 꼭 필요한 parameter인 `in_planes`, `out_planes`, `stride`만을 받아 convolutional layer module를 return해주는 함수를 만들어 사용합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ey5yMjX_S8d"
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):  # stride =1이면 dimension 유지 / default는 1이고 다른값 넣으면 됨\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, \n",
        "                     bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTsbOiknBEBA"
      },
      "source": [
        "### BasicBlock Module\n",
        "2개의 3x3 convolution layer와 skip connection으로 구성된 `BasicBlock` module을 구현해봅시다.\n",
        "\n",
        "[BasicBlock Module Image](https://imgur.com/a/M9gZjWc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa4d5OM7Amkd"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)  # 위의 output이 새로 input\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOst0o4OBYYG"
      },
      "source": [
        "### Bottleneck Module\n",
        "50개 이상의 layer를 가진 ResNet architecture에서 computational efficiency를 증가시키기 위해 3x3 convolution layer 앞뒤로 1x1 convolution layer를 추가한 `Bottleneck` module을 구현해 봅시다.\n",
        "\n",
        "[Bottleneck Module Image](https://imgur.com/a/HrpmJbU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgmSrlL4BXVq"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Mna3wVDOFM"
      },
      "source": [
        "# ResNet Module\n",
        "\n",
        "적절한 Block type과 layer 수, 그리고 최종적으로 분류할 class 갯수를 받아 ResNet architecture를 구현해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F56f7NMSDEYs"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=100, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUkZeRF_D0y1"
      },
      "source": [
        "# Train, Validate, Test and Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AodJ0AufDgmJ"
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDa7F5dyD0S9"
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9aSUcjAD566"
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QyjiV5yD7to"
      },
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net =  ResNet(block = args.block, layers = args.layers, \n",
        "                        num_classes = 100, zero_init_residual = args.zero_init_residual)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGEg-JdZD_od"
      },
      "source": [
        "# Manage Experiment Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHNXD0FLD-RT"
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dedRUkNEGYM"
      },
      "source": [
        "# Visualizatin Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBO5-j6pEBtd"
      },
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "    \n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "    \n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "    \n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeRl-4w3ESID",
        "outputId": "910bd58f-4e60-4756-95f7-bbe1d6239400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr\"\n",
        "\n",
        "args.block = BasicBlock\n",
        "args.layers = [1,1,1,1]\n",
        "args.zero_init_residual = False\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 512\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "\n",
        "setting, result = experiment(partition, deepcopy(args))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Acc(train/val): 7.55/8.12, Loss(train/val) 4.20/5.16. Took 11.69 sec\n",
            "Epoch 1, Acc(train/val): 14.17/15.60, Loss(train/val) 3.62/3.55. Took 11.60 sec\n",
            "Epoch 2, Acc(train/val): 18.71/19.82, Loss(train/val) 3.36/3.34. Took 11.73 sec\n",
            "Epoch 3, Acc(train/val): 23.02/22.62, Loss(train/val) 3.12/3.17. Took 11.64 sec\n",
            "Epoch 4, Acc(train/val): 26.51/25.49, Loss(train/val) 2.95/3.05. Took 11.69 sec\n",
            "Epoch 5, Acc(train/val): 30.03/27.42, Loss(train/val) 2.76/2.94. Took 11.72 sec\n",
            "Epoch 6, Acc(train/val): 33.45/28.29, Loss(train/val) 2.59/2.88. Took 11.70 sec\n",
            "Epoch 7, Acc(train/val): 36.85/28.76, Loss(train/val) 2.43/2.92. Took 11.66 sec\n",
            "Epoch 8, Acc(train/val): 39.87/30.71, Loss(train/val) 2.27/2.91. Took 11.67 sec\n",
            "Epoch 9, Acc(train/val): 43.07/32.90, Loss(train/val) 2.12/2.77. Took 11.70 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZq-8gcsdrXK",
        "outputId": "b7aad86f-8247-4059-c9cf-f981cb3bd51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr\"\n",
        "\n",
        "\n",
        "args.block = Bottleneck\n",
        "args.layers = [1,1,1,1]\n",
        "args.zero_init_residual = False\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 512\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "\n",
        "setting, result = experiment(partition, deepcopy(args))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Acc(train/val): 1.73/1.70, Loss(train/val) 5.54/6.84. Took 25.40 sec\n",
            "Epoch 1, Acc(train/val): 2.29/2.55, Loss(train/val) 4.50/4.52. Took 25.31 sec\n",
            "Epoch 2, Acc(train/val): 3.01/2.97, Loss(train/val) 4.41/4.37. Took 25.49 sec\n",
            "Epoch 3, Acc(train/val): 3.74/3.80, Loss(train/val) 4.31/4.30. Took 25.77 sec\n",
            "Epoch 4, Acc(train/val): 5.21/5.63, Loss(train/val) 4.16/4.12. Took 25.75 sec\n",
            "Epoch 5, Acc(train/val): 6.79/7.22, Loss(train/val) 4.01/3.98. Took 25.79 sec\n",
            "Epoch 6, Acc(train/val): 8.61/9.19, Loss(train/val) 3.87/3.86. Took 26.00 sec\n",
            "Epoch 7, Acc(train/val): 10.95/10.69, Loss(train/val) 3.75/3.77. Took 26.50 sec\n",
            "Epoch 8, Acc(train/val): 13.21/13.82, Loss(train/val) 3.63/3.62. Took 27.62 sec\n",
            "Epoch 9, Acc(train/val): 15.56/16.17, Loss(train/val) 3.50/3.51. Took 27.37 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-h9BEv4n2xg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}